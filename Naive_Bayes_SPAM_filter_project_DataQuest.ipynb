{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes SPAM filter project - DataQuest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF1ER29tM7oJ",
        "colab_type": "text"
      },
      "source": [
        "# Building a SMS Spam Filter with Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YINWwjfLNB5w",
        "colab_type": "text"
      },
      "source": [
        "##Introduction\n",
        "This work is an exercise of the machine learning course of Data Quest. The aim is to build a good SMS Spam classifier with Naive Bayes algorithm and the database available in [this url](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition).\n",
        "\n",
        "** Para os que falam português e leem esse trabalho, não fiz um classificador de Spam para mensagens escritas em lingua portugues pois o curso **DataQuest** é norte-americano e explora conteúdo na lingua inglesa. Apesar disso, a lógica de código usada nesse projeto é capaz de criar um filtro de Spam em qualquer idioma, desde que empossado de uma boa base de dados.\n",
        "\n",
        "You can learn more about this method [here](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) or [here in portuguese](https://www.organicadigital.com/blog/algoritmo-de-classificacao-naive-bayes/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpoXXOHpmHDa",
        "colab_type": "text"
      },
      "source": [
        "## Conclusions\n",
        "In this work, we build a SPAM SMS classifier with 98.74% accuracy and 95% F1_score using the Multinomial Naive Bayes Algorithm. The method is simple and recquire simple and easy to learn concepts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7HG4HJgO2Nx",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp--yLJyIvBa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "897fafa9-8f68-4c1d-ad13-0bebd42c912c"
      },
      "source": [
        "# Essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "#Graphical Libraries\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as pyo\n",
        "import plotly.figure_factory as ff\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVRMP0TbLqo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the data set\n",
        "df = pd.read_table('/content/drive/My Drive/Colab Notebooks/SMSSpamCollection.txt',header = None,names = ['spam_status','SMS_msg'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAluC0HRPVIF",
        "colab_type": "code",
        "outputId": "25b3213d-a3cd-440b-f22b-e9094e36027f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spam_status</th>\n",
              "      <th>SMS_msg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>spam</td>\n",
              "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ham</td>\n",
              "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>spam</td>\n",
              "      <td>WINNER!! As a valued network customer you have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>spam</td>\n",
              "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  spam_status                                            SMS_msg\n",
              "0         ham  Go until jurong point, crazy.. Available only ...\n",
              "1         ham                      Ok lar... Joking wif u oni...\n",
              "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         ham  U dun say so early hor... U c already then say...\n",
              "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
              "5        spam  FreeMsg Hey there darling it's been 3 week's n...\n",
              "6         ham  Even my brother is not like to speak with me. ...\n",
              "7         ham  As per your request 'Melle Melle (Oru Minnamin...\n",
              "8        spam  WINNER!! As a valued network customer you have...\n",
              "9        spam  Had your mobile 11 months or more? U R entitle..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_SHZSH7PVYd",
        "colab_type": "code",
        "outputId": "7005e6c5-3ff9-472d-e30a-4773031194d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df['spam_status'].value_counts(normalize=True)*100"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     86.593683\n",
              "spam    13.406317\n",
              "Name: spam_status, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRmXTiKzS_eF",
        "colab_type": "text"
      },
      "source": [
        "## Splitting Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CEPZ2A2TEN8",
        "colab_type": "text"
      },
      "source": [
        "After designing a Spam classifier, it is essential to assess its performance in a completly new data. We can do it by splitting the dataframe in two parts (not equal). \n",
        "\n",
        "The first part is the **training set**. We use it to calculate the parameters of the Naive Bayes algorithm (the prior and conditional probabilities). The second part is called the **test set** in which we can input the messages in our desgined classifier and evaluate how well it does in classifying unsee messages. \n",
        "\n",
        "This is important in practice because a classifier that performs well in the design phase but do not during test is a bad system! \n",
        "\n",
        "Think about it as a car that was designed in controlled conditions and performs really well in this scenario. But when it goes to the streets, it crashes in rapdly! Would like such a car?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t8BwURiUepG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Randomly sorting the DataFrame and splitting data\n",
        "df_sorted = df.sample(frac=1,random_state = 1) #sorting the dataframe\n",
        "split_idx = round(len(df)*0.8) #Index to split the data frame 80% train\n",
        " \n",
        "train_df = df_sorted.iloc[0:split_idx] #Training dataframe\n",
        "test_df = df_sorted.iloc[split_idx:]   #Test dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dlHRK6cYbEU",
        "colab_type": "code",
        "outputId": "e915a528-f034-4316-ddc6-30057c89304d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Checking the proportion of Spam and Non Spam messages\n",
        "train_df['spam_status'].value_counts(normalize=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     0.86541\n",
              "spam    0.13459\n",
              "Name: spam_status, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMUAbCl2Zm9c",
        "colab_type": "text"
      },
      "source": [
        "The table shows that the proportion of spam to non spam messages is about the same if compared to the entire data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1boOGIU5bSCo",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning in message column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joHhQd74g3mL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #testing scikitlearn\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# x = train_df.message\n",
        "# y= train_df.spam_status\n",
        "# vectorize = CountVectorizer()\n",
        "# a = vectorize.fit_transform(x)\n",
        "# len(vectorize.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL13Z2Zsa4vS",
        "colab_type": "text"
      },
      "source": [
        "In order to build the classifier, a bit of data cleaning is necessary! Here we want a classifier that input words and output the probabilities of being a spam or not.\n",
        "\n",
        "If we take a look at the table, we can see that some messages have ponctuation symbols which are characters we will not use as features build the classifier. So, the first step is to strip those caracters from the message column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw8xIJCtdbSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = train_df.copy()\n",
        "test_df = test_df.copy()\n",
        "train_df['SMS_msg'] = train_df['SMS_msg'].str.replace('\\W',' ').str.lower()\n",
        "#test_df['SMS_msg'] = test_df['SMS_msg'].str.replace('\\W',' ').str.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVUnHiFPjtnF",
        "colab_type": "text"
      },
      "source": [
        "## Building the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-fjcJgJDV29",
        "colab_type": "text"
      },
      "source": [
        "The vocabulary will be useful to calculate the total number of unique words in our data set. In order to this, we must first split the words by the ' ' string in the message column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKieeQAFjvtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting the words in the sentences by the ' ' string\n",
        "vocabulary = []\n",
        "separate_words = train_df['SMS_msg'].str.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OeMHzEbDHha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building a nested loop to append every word from every column in the 'message' column\n",
        "for i in separate_words:\n",
        "    for j in i:\n",
        "        vocabulary.append(j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg2IO2SQETt0",
        "colab_type": "code",
        "outputId": "1591d210-855f-4184-c11d-cf77c4db72e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Using the set function to eliminate duplicate entries - A set is defined as a collection of non repeated objects\n",
        "unique_vocabulary = set(vocabulary)\n",
        "print('the size of the list with all words is',len(vocabulary))\n",
        "print('\\n')\n",
        "print('the size of the set with all unique words is',len(unique_vocabulary))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the size of the list with all words is 72427\n",
            "\n",
            "\n",
            "the size of the set with all unique words is 7783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MAZ9rBuEWoA",
        "colab_type": "code",
        "outputId": "8bb928a3-76b9-4adb-9560-7a7d87952140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unique_vocabulary = list(unique_vocabulary)\n",
        "unique_vocabulary = [x for x in unique_vocabulary if x != \" \"]\n",
        "len(unique_vocabulary)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7783"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha9CPXYCHqsZ",
        "colab_type": "text"
      },
      "source": [
        "## Creating a dataframe to be used as a supervised learning problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMTgVd54H5dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize a dictionary with the same values equal to a zero vector with the same size as the number of rows in the train data set\n",
        "word_counts_per_sms = {unique_word: [0] * len(train_df['SMS_msg']) for unique_word in unique_vocabulary}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xbiRXwbtHco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## VERY IMPORTANT TO BREAK A STRING INTO THEIR SEPARATED WORDS\n",
        "train_df['SMS_msg'] = train_df['SMS_msg'].str.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyK457p1IgtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index,sms in enumerate(train_df['SMS_msg']):\n",
        "    for word in sms:\n",
        "        word_counts_per_sms[word][index] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50FJiAGgtZby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforming the features into a DataFrame\n",
        "feature_df = pd.DataFrame(word_counts_per_sms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKnPxbl3zQ6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG1OlgpGth1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Concatenating with the original training data set\n",
        "new_train = pd.concat([train_df,feature_df],join='outer',axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaPrQAA81bYq",
        "colab_type": "code",
        "outputId": "3d020c90-d40d-4533-ee37-aad5a6789c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "new_train.head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>spam_status</th>\n",
              "      <th>SMS_msg</th>\n",
              "      <th>bless</th>\n",
              "      <th>program</th>\n",
              "      <th>duchess</th>\n",
              "      <th>aretaking</th>\n",
              "      <th>lacs</th>\n",
              "      <th>evil</th>\n",
              "      <th>08715705022</th>\n",
              "      <th>timi</th>\n",
              "      <th>nevering</th>\n",
              "      <th>mesages</th>\n",
              "      <th>fassyole</th>\n",
              "      <th>bawling</th>\n",
              "      <th>sachin</th>\n",
              "      <th>sharing</th>\n",
              "      <th>ability</th>\n",
              "      <th>60</th>\n",
              "      <th>kiosk</th>\n",
              "      <th>____</th>\n",
              "      <th>contribute</th>\n",
              "      <th>voda</th>\n",
              "      <th>find</th>\n",
              "      <th>tonexs</th>\n",
              "      <th>yavnt</th>\n",
              "      <th>blowing</th>\n",
              "      <th>replied</th>\n",
              "      <th>zac</th>\n",
              "      <th>brownies</th>\n",
              "      <th>smoking</th>\n",
              "      <th>ay</th>\n",
              "      <th>mys</th>\n",
              "      <th>bao</th>\n",
              "      <th>fellow</th>\n",
              "      <th>amongst</th>\n",
              "      <th>2nd</th>\n",
              "      <th>role</th>\n",
              "      <th>valentines</th>\n",
              "      <th>hol</th>\n",
              "      <th>...</th>\n",
              "      <th>against</th>\n",
              "      <th>msg150p</th>\n",
              "      <th>cheating</th>\n",
              "      <th>2wks</th>\n",
              "      <th>inshah</th>\n",
              "      <th>bad</th>\n",
              "      <th>08712103738</th>\n",
              "      <th>revealed</th>\n",
              "      <th>09066364349</th>\n",
              "      <th>smsco</th>\n",
              "      <th>mojibiola</th>\n",
              "      <th>locks</th>\n",
              "      <th>perfume</th>\n",
              "      <th>meg</th>\n",
              "      <th>eatin</th>\n",
              "      <th>rumour</th>\n",
              "      <th>fats</th>\n",
              "      <th>fridge</th>\n",
              "      <th>wit</th>\n",
              "      <th>gonnamissu</th>\n",
              "      <th>pass</th>\n",
              "      <th>tallahassee</th>\n",
              "      <th>wewa</th>\n",
              "      <th>nd</th>\n",
              "      <th>opinion</th>\n",
              "      <th>your</th>\n",
              "      <th>sitting</th>\n",
              "      <th>fathima</th>\n",
              "      <th>success</th>\n",
              "      <th>neva</th>\n",
              "      <th>darling</th>\n",
              "      <th>grace</th>\n",
              "      <th>69669</th>\n",
              "      <th>school</th>\n",
              "      <th>subtoitles</th>\n",
              "      <th>banter</th>\n",
              "      <th>laughing</th>\n",
              "      <th>espe</th>\n",
              "      <th>situations</th>\n",
              "      <th>apologize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1078</td>\n",
              "      <td>ham</td>\n",
              "      <td>[yep, by, the, pretty, sculpture]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4028</td>\n",
              "      <td>ham</td>\n",
              "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>958</td>\n",
              "      <td>ham</td>\n",
              "      <td>[welp, apparently, he, retired]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4642</td>\n",
              "      <td>ham</td>\n",
              "      <td>[havent]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4674</td>\n",
              "      <td>ham</td>\n",
              "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7786 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   index spam_status  ... situations  apologize\n",
              "0   1078         ham  ...          0          0\n",
              "1   4028         ham  ...          0          0\n",
              "2    958         ham  ...          0          0\n",
              "3   4642         ham  ...          0          0\n",
              "4   4674         ham  ...          0          0\n",
              "\n",
              "[5 rows x 7786 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liCQaHTV3Ov-",
        "colab_type": "text"
      },
      "source": [
        "## Estimating the prior Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5JgsLrd15GZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_ham = new_train['spam_status'].value_counts(normalize=True)['ham'] # Probability of getting a spam email\n",
        "p_spam = new_train['spam_status'].value_counts(normalize=True)['spam'] #Probability of not getting a spam email\n",
        "\n",
        "# Calculating the number of words in spam emails\n",
        "spam_df = new_train[new_train['spam_status'] == 'spam'] #Filtering rows that contains spam emails\n",
        "n_spam = spam_df['SMS_msg'].apply(len).sum()\n",
        "\n",
        "#calculating the number of words in non spam emails\n",
        "non_spam_df = new_train[new_train['spam_status'] == 'ham'] #Filtering rows that contains spam emails\n",
        "n_non_spam = non_spam_df['SMS_msg'].apply(len).sum()\n",
        "n_vocab = len(unique_vocabulary)\n",
        "alpha=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymXu6xvwN5Bc",
        "colab_type": "text"
      },
      "source": [
        "## Conditional Probabilities Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT9i2VzaN-fB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pes_spam = {word:0 for word in unique_vocabulary}\n",
        "pes_ham = {word:0 for word in unique_vocabulary}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGZ76EaNO1kJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in unique_vocabulary:\n",
        "    prob_word_spam = (spam_df[word].sum() + alpha)/(n_spam + n_vocab*alpha)\n",
        "    prob_word_non_spam = (non_spam_df[word].sum() + alpha)/(n_non_spam + n_vocab*alpha)\n",
        "    pes_spam[word]=prob_word_spam\n",
        "    pes_ham[word] = prob_word_non_spam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RekNKRVScvVz",
        "colab_type": "text"
      },
      "source": [
        "## Deploying the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytVvN9pxZndj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def classify_test_set(message):\n",
        "\n",
        "    message = re.sub('\\W', ' ', message)\n",
        "    message = message.lower()\n",
        "    message = message.split()\n",
        "\n",
        "        \n",
        "    #This is where we calculate:\n",
        "\n",
        "    p_spam_given_message = p_spam\n",
        "    p_ham_given_message = p_ham\n",
        "\n",
        "    for word in message:\n",
        "        if word in unique_vocabulary:\n",
        "           p_spam_given_message *= pes_spam[word]\n",
        "           p_ham_given_message *=pes_ham[word]\n",
        "\n",
        "\n",
        "    #print('P(Spam|message):', p_spam_given_message)\n",
        "    #print('P(Ham|message):', p_ham_given_message)\n",
        "\n",
        "    if p_ham_given_message > p_spam_given_message:\n",
        "        return('ham')\n",
        "    elif p_ham_given_message < p_spam_given_message:\n",
        "        return('spam')\n",
        "    else:\n",
        "        return 'needs human classification'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gblZiz0jkAuD",
        "colab_type": "text"
      },
      "source": [
        "## Assessing the algorithm with the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVAVGZQ1gmvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a590bb68-1645-4be0-887a-6c16f968c490"
      },
      "source": [
        "test_df['predicted'] = test_df['SMS_msg'].apply(classify_test_set)\n",
        "test_df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spam_status</th>\n",
              "      <th>SMS_msg</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2131</th>\n",
              "      <td>ham</td>\n",
              "      <td>Later i guess. I needa do mcat study too.</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3418</th>\n",
              "      <td>ham</td>\n",
              "      <td>But i haf enuff space got like 4 mb...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3424</th>\n",
              "      <td>spam</td>\n",
              "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1538</th>\n",
              "      <td>ham</td>\n",
              "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5393</th>\n",
              "      <td>ham</td>\n",
              "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     spam_status                                            SMS_msg predicted\n",
              "2131         ham          Later i guess. I needa do mcat study too.       ham\n",
              "3418         ham             But i haf enuff space got like 4 mb...       ham\n",
              "3424        spam  Had your mobile 10 mths? Update to latest Oran...      spam\n",
              "1538         ham  All sounds good. Fingers . Makes it difficult ...       ham\n",
              "5393         ham  All done, all handed in. Don't know if mega sh...       ham"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TseMYDzzlTJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c32b539-db52-4ea2-d522-6e2857b1099f"
      },
      "source": [
        "boolean_acc = test_df['predicted'] == test_df['spam_status']\n",
        "counts = boolean_acc.value_counts()\n",
        "accuracy = counts[True]/counts.sum()\n",
        "accuracy"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9874326750448833"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82pP5W0cnf6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boolean_recall =  (test_df.loc[test_df['spam_status']=='spam','predicted']) == (test_df.loc[test_df['spam_status']=='spam','spam_status'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75wpUrnQsIB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "111e6e32-298a-4d56-b4f2-12de2e2d6248"
      },
      "source": [
        "boolean_recall.value_counts()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     139\n",
              "False      8\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSlcRdNpsKFU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0760f438-8930-4e1c-948d-86782aa4ca80"
      },
      "source": [
        "recall = boolean_recall.value_counts()[True]/boolean_recall.value_counts().sum()\n",
        "recall"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9455782312925171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcRn_fWVsV2a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b229b49c-8d90-4881-9328-8c67635c08e7"
      },
      "source": [
        "boolean_precision = (test_df.loc[test_df['predicted']=='spam','predicted']) == (test_df.loc[test_df['predicted']=='spam','spam_status'])\n",
        "precision =  boolean_precision.value_counts()[True]/boolean_precision.value_counts().sum()\n",
        "precision"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9652777777777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TG-rIcBt1xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "F1_score = 2*precision*recall/(precision+recall)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veiU8LmvuHZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d99e167-5974-4c59-90cc-0bbead259722"
      },
      "source": [
        "F1_score"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9553264604810997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}